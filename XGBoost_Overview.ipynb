{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from statsmodels.stats.diagnostic import linear_harvey_collier\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "pd.options.display.max_columns = None  # display all columns when I print a dataframe\n",
    "pd.options.display.max_seq_items = 5000 # display larger numbers of rows in pandas when I print out a Series or Index\n",
    "pd.options.display.max_rows = 5000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Overview\n",
    "**What is Boosting?**\n",
    "- Not a specific machine learning algorithm\n",
    "- Concept that can be applied to a set of machine learning models\n",
    "- \"Meta-algorithm\": Ensemble meta-algorithm used to convert many weak learners into a strong learner\n",
    "\n",
    "**Weak learners and strong learners**\n",
    "- Weak learner: ML algorithm that is slightly better than chance\n",
    "- Boosting converts a collection of weak learners into a strong learner\n",
    "- Strong learner: Any algorithm that can be tuned to achieve good performance.\n",
    "\n",
    "**How boosting is accomplished?**\n",
    "- Iteratively learning a set of week models on subsets of the data\n",
    "- Weighting each weak prediction according to each weak learner's performance\n",
    "- Combine the weighted predictions to obtain a single weighted prediction\n",
    "- that is much better than the individual predictions themselves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Types\n",
    "\n",
    "- Gradient Boosted Trees https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "- Histogram-based Gradient Boosting Regression Tree: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor\n",
    "- XG Boost (https://xgboost.readthedocs.io/en/stable/index.html)\n",
    "- LightGBM (https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/)\n",
    "- CatBoost\n",
    "- AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Parameters**\n",
    "- `booster` (default=gbtree) \n",
    "\n",
    "- `verbosity` (default=1): Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug)\n",
    "\n",
    "- `early_stopping_rounds`: related to `num_boosting_round`. Instead of specificying a hard value of `num_boosting_round`, we use early stopping. If the hold-out  metric on a hold-out set does not change for the # of `early_stopping_rounds` then we stop optimization. If the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur.\n",
    "\n",
    "**Parameters for Tree Booster**\n",
    "- `n_estimators`: specifies how many sequential trees we want to make that attempt to correct for prior trees. Also, called Number of boosting rounds `num_boost_round`.\n",
    "\n",
    "- `learning_rate` [default=0.3, eta]: associated with each tree/boosting round on how we calculate residual. Y = Y - alpha[k] * learner[k].predict(X) for the $k^{th}$ tree and learner is the tree we build. \n",
    "\n",
    "- `min_samples_split`: is the minimum weight (or number of samples if all samples have a weight of 1) required in order to create a new node in the tree. A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit. DOES NOT EXISTS IN SKLEARN API\n",
    "\n",
    "- `max_depth` [default=6]: Depth of each tree. \n",
    "\n",
    "- `max_leaves`: maximum number of leaves, 0 indicates no limit\n",
    "\n",
    "- `colsample_bytree`: fraction of features to choose from at every split in a given tree. Subsampling occurs once for every tree constructed. Others are `colsample_bylevel`, `colsample_bynode`.\n",
    "\n",
    "- `min_child_weight`: The number of samples required to form a leaf node (the end of a branch). A leaf node is the termination of a branch and therefore the decision node of what class a sample belongs to.\n",
    "\n",
    "- `subsample` [default=1]: corresponds to the fraction of observations (the rows) to subsample at each step. By default it is set to 1 meaning that we use all rows. Subsample occurs once in every boosting iteration. Subsample - 0.5 means xgboost would randomly sampple half training data prior to growing trees. \n",
    "\n",
    "- `scale_pos_weight`: Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "\n",
    "- `sampling_method`: The method to use to sample the training instances. This has two values: `uniform`, `gradient_based` \n",
    "\n",
    "- `max_delta_step`\n",
    "\n",
    "- `tree_method`: tree construction method. \n",
    "\n",
    "- `eval_metric`: evaluation metric for validation dataset, rmse for regression, logloss for classification. \n",
    "\n",
    "**Regularization in XG Boost (https://github.com/goodboychan/goodboychan.github.io/blob/main/_notebooks/2020-07-07-02-Fine-tuning-your-XGBoost-model.ipynb)**\n",
    "- `gamma` [default=0, alias: min_split_loss:] minimum loss reduction to create a new tree split \n",
    "\n",
    "- `alpha`: L1 reg on leaf weights (For tree booster).,  L1 reg on weights (For linear booster). \n",
    "\n",
    "- `lambda`: L2 reg on leaf weights (For tree booster).,  L2 reg on weights. For linear booster. \n",
    "\n",
    "\n",
    "**Base Learners in XG boost**\n",
    "- Linear Base Learners: Linear Regression with both L1 and L2 regularization (Elastic Net ) as base leaner instead of decision tree. \n",
    "- Tree Base Learners: the base learner is gbtree, short for “gradient boosted tree,” which operates like a standard sklearn Decision Tree.\n",
    "\n",
    "**Learning Task Parameters**\n",
    "- `objective`: `reg:squaredloss` regression with squared loss, `binary:logistic` logistic regression for binary classification, output is probability \n",
    "- `binary:logistic` is the default objective for XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- https://kevinvecmanis.io/machine%20learning/hyperparameter%20tuning/dataviz/python/2019/05/11/XGBoost-Tuning-Visual-Guide.html\n",
    "- https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/07/02-Fine-tuning-your-XGBoost-model.html\n",
    "- https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
